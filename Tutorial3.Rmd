---
title: "Tutorial 3"
author: Matthias Nijman - S4667999
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

## 1
### 1A
| i/j   | 1     | 2     | 3     | 4     | 5     | 6     | P(X=i)|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 1     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 2     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 3     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 4     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 5     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 6     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| P(Y=j)| 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1     |

Since $P(X = i, Y = j) = P(X = i)P(Y = j)$, X and Y are independent.

### 1B
$P(Z=1) = P(X=1) = 1/6$\
$P(Z=2) = P(X=1) = 1/6$\
$P(Z=3) = P(X=1) = 1/6$\
$P(Z=4) = P(X>3)(P(Y=4)+P(Y=1)) = 1/2(1/6 + 1/6)=1/6$\
$P(Z=5) = P(X>3)(P(Y=5)+P(Y=2)) = 1/2(1/6 + 1/6)=1/6$\
$P(Z=6) = P(X>3)(P(Y=6)+P(Y=3)) = 1/2(1/6 + 1/6)=1/6$

| i/j   | 1     | 2     | 3     | 4     | 5     | 6     | P(X=i)|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 1     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 2     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 3     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 4     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 5     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 6     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| P(Z=j)| 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1     |

Since $P(X = i, Z = j) = P(X = i)P(Z = j)$, X and Z are also independent.

## 2
### 2A
I don't understand.\
<span style="color:red">
$\int_{-\pi/2}^{\pi/2} f(x)\,dx = 1$\
$\int_{-\pi/2}^{\pi/2} k\cos(x)\,dx= k[\sin(x)]_{-\pi/2}^{\pi/2}= k(1 - (-1))= 2k$\
$2k = 1$ so $k = \frac{1}{2}$
</span>

### 2B
$0$ is between $-\pi/2$ and $\pi/2$.\
So $f(x) \geq kcos(x)$\
$f(x) \geq \frac{1}{2}cos(0)$\
$f(x) \geq \frac{1}{2}$

### 2C
$1$ is between $-\pi/2$ and $\pi/2$.\
So $f(x) \leq kcos(x)$\
$f(x) \leq \frac{1}{2}cos(1)$\
<span style="color:red">
*Learn integrals.*
</span>

## 3
For $X=1$, Y can be anything from 2 up to 7 (1 + outcome of the die). For $X=-1$, Y can be anything from 0 up to 5 (-1 + outcome of the die).
These chances are equally divided: $P(\omega_c) \times P(\omega_d) = \frac{1}{2} \times \frac{1}{6} = \frac{1}{12}$

| i/j   | 0     | 1     | 2     | 3     | 4     | 5     | 6     | 7     | P(X=i)|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 1     | 0     | 0     | 1/12  | 1/12  | 1/12  | 1/12  | 1/12  | 1/12  | 1/2   |
| -1    | 1/12  | 1/12  | 1/12  | 1/12  | 1/12  | 1/12  | 0     | 0     | 1/2   |
| P(Y=j)| 1/12  | 1/12  | 1/6   | 1/6   | 1/6   | 1/6   | 1/12  | 1/12  | 1     |

### 3A
$P(Y|X=1)$ is the first row of the table.

### 3B
$P(X|Y=0)$ is the first column of the table.

### 3C
$P(X|Y=7)$ is the last column of the table.

### 3D
$P(X|Y=3)$ is the 4th column of the table.

### 3E
For X and Y to be independent, the following equation must hold: $P(X = i, Y = j) = P(X = i)P(Y = j)$.
We can see that $P(X = 1, Y = 0) = 0$ while $P(X = 1)P(Y = 0) = \frac{1}{2} \times \frac{1}{12} = \frac{1}{24}$. Thus X and Y are not independent.

## 4
### 4A
| 1     | 2     | 3     | 4     | 5     | 6     | P(X=i)|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1     |

| 1     | 2     | 3     | 4     | 5     | 6     | P(Y=i)|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1     |

They have the same PMF table. Thus the same probability distribution.

### 4B
If we can prove that X and Y are independent, we know that they are not equal.
Thus we can create a PMF table of the two and see if they are independent.

| i/j   | 1     | 2     | 3     | 4     | 5     | 6     | P(X=i)|
| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| 1     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 2     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 3     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 4     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 5     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| 6     | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/36  | 1/6   |
| P(Y=j)| 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1/6   | 1     |

Since $P(X = i, Z = j) = P(X = i)P(Y = j)$, X and Y are independent.

## 5
A fair coin flip where you gain 1 EUR when flipping H, and lose 1 EUR when T comes up.
$\mathbb{E}[X] = 1P(X = 1) + (−1)P(X = −1) = 1 \times \frac{1}{2} - 1 \times \frac{1}{2} = 0$

Has a different probability distribution but the same expected value as a fair dice roll where:
$$
X(\omega)=
\begin{cases}
-3, & \omega = 1 \\
-2, & \omega = 2 \\
-1, & \omega = 2 \\
1, & \omega = 3 \\
2, & \omega = 4 \\
3, & \omega = 5 \\
\end{cases}
$$
$\mathbb{E}[X] = (-3)P(X = 1) + (−2)P(X = 2) + (−1)P(X = 3) + 1P(X = 4) + 2P(X = 5) + 3P(X = 6) =$
$-3 \times \frac{1}{6} - 2 \times \frac{1}{6} -1 \times \frac{1}{6} + 1 \times \frac{1}{6} + 2 \times \frac{1}{6} + 3 \times \frac{1}{6} = 0$

## 6
### 6A
```{r}
flip_until_heads <- function() {
  toss <- ""
  count <- 0
  while(toss != "H"){
    count <- count + 1
    toss <- sample(x=c("H","T"), size=1)
  }
  return (2^count)
}
```
### 6B
```{r}
set.seed(1)
results_100 <- replicate(100, flip_until_heads())
results_1000 <- replicate(1000, flip_until_heads())
results_10000 <- replicate(10000, flip_until_heads())
results_100000 <- replicate(100000, flip_until_heads())
```
### 6C
```{r}
mean(results_100)
mean(results_1000)
mean(results_10000)
mean(results_100000)
```
Theory suggests that the more we play this game, the more our winnings go up. However, we see that we got roughly:\
8 on average for 100 tosses\
11 on average for 1000 tosses\
42 on average for 10.000 tosses\
16 on average for 10.0000 tosses\
I suppose we got lucky on the 10.000 throws sample.\
<span style="color:red">
*The $\mathbb{E}[X]$ here is $\infty$, thus the outcome is for from the theory. (I looked a little to far in the theory (At the paradox) and past this part)*
</span>

### 6D-i
It does not really change much. $\mathbb{E}[X] = \infty - 5$, which is still infinity.

### 6D-i,ii,iii
```{r}
flip_until_heads <- function() {
  toss <- ""
  count <- 0
  while(toss != "H"){
    count <- count + 1
    toss <- sample(x=c("H","T"), size=1)
  }
  return (-5 + 2^count)
}

set.seed(1)
results_100 <- replicate(100, flip_until_heads())
results_1000 <- replicate(1000, flip_until_heads())
results_10000 <- replicate(10000, flip_until_heads())
results_100000 <- replicate(100000, flip_until_heads())

mean(results_100)
mean(results_1000)
mean(results_10000)
mean(results_100000)
```

### 6D-iv
The outcomes with the same seed are exactly as expected: $outcome - 5$, which is till a positive number in all of our test cases.
Thus the game does remain paradoxical.

## 7
$$
X(\omega)=
\begin{cases}
Red, & \omega = -100 \\
Blue, & \omega = 0 \\
Green, & \omega = 50 \\
Yellow, & \omega = 100 \\
\end{cases}
$$

### 7A
$\mathbb{E}[X] = (-100)P(X = R) + 0P(X = B) + 50P(X = G) + 100P(X = Y) = -100 \times \frac{4}{10} + 0 \times \frac{2}{10} + 50 \times \frac{3}{10} + 100 \times \frac{1}{10} = -15$
<span style="color:red">
*The question was to calculate return not gain. Make sure to double check wordings!*
</span>

### 7B
$var(X) = \mathbb{E}[(X-\mathbb{E}[X])^2)]$\
$\mathbb{E}[X] = (-100 + 15)^2P(X = R) + (0-15)^2P(X = B) + (50-15)^2P(X = G) + (100-15)^2P(X = Y) =$\
$-85^2 \times \frac{4}{10} -15^2 \times \frac{2}{10} + 35^2 \times \frac{3}{10} + 85^2 \times \frac{1}{10} = -1845$

```{r}
-85^2*4/10 -15^2*2/10 + 35^2*3/10 + 85^2*1/10
```

### 7C
```{r}
draw_ball <- function() {
  draw <- sample(x=c("R","B","G","Y"), size = 1, prob=c(0.4, 0.2, 0.3, 0.1))
  if (draw == "R")  return(-100)
  if (draw == "B")  return(0)
  if (draw == "G")  return(50)
  if (draw == "Y")  return(100)
}

estimate_avg_for_n <- function(n) {
  return(mean(replicate(n, draw_ball())))
}
```

### 7D
```{r}
set.seed(1)
results <- sapply(1:1000, estimate_avg_for_n)
plot(results, type='l', main="Draw balls experiment", xlab="Games played", ylab="Average return")
abline(h = -15, col = "blue")
```
The average return seems to stabilize around -15. Which is exactly as we calculated.